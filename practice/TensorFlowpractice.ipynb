{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#tensor = array \\nmy_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] #3x3 \\na = torch.tensor(my_list)\\nprint(a.shape)\\nprint(a.dtype)\\nprint(a @ a)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\"\"\"\n",
    "#tensor = array \n",
    "my_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] #3x3 \n",
    "a = torch.tensor(my_list)\n",
    "print(a.shape)\n",
    "print(a.dtype)\n",
    "print(a @ a)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#linear layer \\nlinear_layer = nn.Linear(\\n    in_features = 3, # have to be correct to pass through \\n    out_features = 2 ) # input * weights + bias = output W*x + b\\n#passing through linear layer \\noutput = linear_layer(input_tensor)\\nprint(output)\\n\\nprint(linear_layer.weight)\\nprint(linear_layer.bias)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input layer (number of neurons in input = # of features in dataset)\n",
    "input_tensor = torch.tensor(\n",
    "    [[1.2, 2.2, 3.3333, 5.333, 3.2, 6.3, 7.3, 8.3, 9.2, 10.22]] # 3 neurons in input layer\n",
    ")\n",
    "'''\n",
    "#linear layer \n",
    "linear_layer = nn.Linear(\n",
    "    in_features = 3, # have to be correct to pass through \n",
    "    out_features = 2 ) # input * weights + bias = output W*x + b\n",
    "#passing through linear layer \n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "tensor([[0.7685, 0.9002, 0.9656, 0.9952, 0.9608, 0.9982, 0.9993, 0.9998, 0.9999,\n",
      "         1.0000]])\n",
      "tensor([[7.6037e-05, 2.0669e-04, 6.4196e-04, 4.7420e-03, 5.6184e-04, 1.2472e-02,\n",
      "         3.3902e-02, 9.2154e-02, 2.2666e-01, 6.2858e-01]])\n",
      "tensor([[0.2498]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwoongyoon/Documents/Res/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[[1.2, 2.2, 3.3333, 5.333, 3.2, 6.3, 7.3, 8.3, 9.2, 10.22]] # 10 neurons in input layer\n",
    "\n",
    "'''\n",
    "#Creating more hidden layers & forward pass\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10,5), #1st hidden layer = 5 (11*5 = 55 parameters)\n",
    "    nn.Linear(5,1), #2nd hidden layer = 1 (6 * 1)\n",
    "    #nn.Sigmoid()\n",
    "    #nn.Sigmoid() #binary classification \n",
    "    #nn.Softmax() #multiclass classification\n",
    ")\n",
    "\n",
    "#total = 61 parameters \n",
    "total = 0    \n",
    "#number of parameters \n",
    "for p in model.parameters():\n",
    "    total += p.numel()\n",
    "print(total)\n",
    "\n",
    "#activation functions for classification\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "probability2 = softmax(input_tensor)\n",
    "\n",
    "print(probability)\n",
    "print(probability2)\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "#prediction loss function\n",
    "#one-hot encoding to convert y to a tensor of zeros and ones: ground truth \n",
    "from torch.nn import CrossEntropyLoss\n",
    "y=[1]\n",
    "\n",
    "#loss function: before the final activation function \n",
    "#loss = F(y, y') y' is prediction value y is truth \n",
    "#loss function outputs loss - a float \n",
    "criterion = CrossEntropyLoss()\n",
    "#loss = criterion(pred, target)\n",
    "#loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backprop e.g. gradient descent, mle/optimizers\n",
    "import torch.optim\n",
    "#optimizer SGD: finding minimum error function \n",
    "optimizer = torch.optim(model.parameters(), lr= 0.001)\n",
    "#loss = criterion(pred, target) #cross-entropy\n",
    "#loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]]\n",
      "[0 0 1 1 2]\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "tensor([1, 0])\n",
      "tensor([[2],\n",
      "        [0]])\n",
      "tensor([2, 0])\n",
      "tensor([[2]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import TensorDataset #to store x, y as tensors\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#animal data\n",
    "df = pd.read_csv('OlympicTV.csv')\n",
    "features = df.iloc[:, 1: -1 ]\n",
    "\n",
    "X = features.to_numpy()\n",
    "print(X)\n",
    "\n",
    "#class label for each animal\n",
    "target = df.iloc[:, -1]\n",
    "Y = target.to_numpy()\n",
    "print(Y)\n",
    "\n",
    "#x,y to tensor\n",
    "dataset = TensorDataset(torch.tensor(X), torch.tensor(Y))\n",
    "\n",
    "batch_size = 2\n",
    "shuffle = True \n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "#Iterate over dataloader \n",
    "for batch_inputs, batch_labels in dataloader: \n",
    "    print(batch_inputs)\n",
    "    print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Training Loop \n",
    "# 1. Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    \n",
    "    # 2. Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 3. Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    \n",
    "    # 4. Compute the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    \n",
    "    # 5. Compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # 6. Update the model's parameters\n",
    "    optimizer.step()\n",
    "\n",
    "show_results(model, dataloader)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions \n",
    "'''\n",
    "nn.ReLU()\n",
    "leaky_relu = nn.LeakyReLU(negative_slope = 0.05)\n",
    "\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "x_pos = torch.tensor(2.0)\n",
    "x_neg = torch.tensor(-3.0)\n",
    "\n",
    "output_pos = relu_pytorch(x_pos)\n",
    "output_neg = relu_pytorch(x_neg)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Initialization:\n",
    "Data normalization: scales the input features for stability \n",
    "similarly, a layer's weights are initialized to small values - > layer initialization\n",
    "\n",
    "Transfer Learning: -> same a reduction\n",
    "instead of randomizaing weights -> reusing a model trained on a first task for a second similar task \n",
    "ex. training a model on US army salary - > get weights -> use those weights to train model on EU army salary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(64, 128)\n",
    "print(layer.weight.min(), layer.weight.max()) #initialization the weights to small values to ensure stable outputs\n",
    "nn.init.uniform_(layer.weight) #pytorch for uniform distribution of initialization\n",
    "#transfer learning\n",
    "#to save and load weights \n",
    "torch.save(layer, 'layer.pth')\n",
    "new_layer = torch.load('layer.pth')\n",
    "\n",
    "#Fine-tuning: type of transfer learning \n",
    "#load weights from previously trained model, but train model with a smaller learning rate \n",
    "#smaller learning rate & train part of network (freeze) \n",
    "for name, param in model.named_parameters(): \n",
    "    if name == '0.weight':\n",
    "        param.requires_grad = False #setting the grad for the first layer as false "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Model: step after training \n",
    "- data split into 3 parts\n",
    "  - train: adjust parameters\n",
    "  - validation: tunes hyperparameters (lr & momentum)\n",
    "  - test: evaluates final model performance \n",
    "-tracking loss and accuracy during training and validation \n",
    "\n",
    "training loss and validation loss helps detect overfitting. \n",
    "When model overfits -> train loss decreases but validation loss rise because model becomes used to train data. \n",
    "Thus, the same model when trained on the validation dataset, the loss increases as the model has not learned the validation data. \n",
    "\n",
    "Loss -> describes how well the model is learning the data: ex. loss = MSE(), \n",
    "but not always how acccurately it makes predictions \n",
    "\n",
    "so we use accuracy: \n",
    "(metric= torchmetric.Accuracy(tasks = \"multiclass\", num_classes))\n",
    "metric.update(outputs, labels.argmax(dim=-1)) #we basically keep updating the accuracy for each output (pred), and label (target)\n",
    "#for the whole epoch \n",
    "accuracy = metric.compute()\n",
    "#reset the metric for the next epoch \n",
    "metric.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#1. Calculate the training loss \n",
    "#initialize training loss \n",
    "training_loss = 0.0 \n",
    "\n",
    "for n in epochs: \n",
    "    for inputs, labels in trainloader(): \n",
    "    \n",
    "    #forward pass \n",
    "    outputs = model(inputs)\n",
    "\n",
    "    #loss function\n",
    "    criterion = mean()\n",
    "    loss = criterion(ouputs, labels)\n",
    "     \n",
    "    #backpropagation: gradient descent \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #calculate and sum the loss\n",
    "    training_loss += loss.item() #.item() adds each value to total \n",
    "\n",
    "epoch_loss = training_loss/len(trainloader) #dividing by number of batches in train loader \n",
    "\n",
    "#Calculating Validation Loss \n",
    "validation_loss = 0.0 \n",
    "model.eval() #setting the model to evaluate mode \n",
    "\n",
    "with torch.no_grad(): #disable grad. \n",
    "    for inputs, labels in validationloader: \n",
    "    #forward pass \n",
    "        outputs = model(inputs)\n",
    "    #loss\n",
    "        loss = criterion(ouputs, labels)\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "epoch_loss = validation_loss / len(validationloader)\n",
    "model.train() #switch back to train mode \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting\n",
    "- reduce model size or add dropout layer \n",
    "- use weight decay to force parameters to remain small \n",
    "- obtain new data or augmenting data \n",
    "\n",
    "Regularization: dropout layer\n",
    "- randomly zeroes out elements of the input tensor during training  \n",
    "- added after activation\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5) #50% probability of zeroing out elements\n",
    ")\n",
    "\n",
    "features = ...\n",
    "#dropout zeroes out the feature values \n",
    "\n",
    "Regularization: weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "-encourages smaller weights by adding a penalty during optimization \n",
    "-reduce overfitting, keeping weights smaller "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "Step 1: Overfit the training set (optimizers like gradient descent)\n",
    "step 2: reduce overfitting by increasing performance on the validation set (Regularization) Balance training and validation data\n",
    "Step 3: fine-tune the hyperparameters (lr & momentum) \n",
    "random search: \n",
    "factor = np.random.uniform(2,6)\n",
    "lr = 10 ** -factor \n",
    "grid search: \n",
    "for factor in range(2,6): \n",
    "    lr = 10 ** -factor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
